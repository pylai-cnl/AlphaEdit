{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27a5659b94bc4880936573df482a5520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_f98a24f661e544eaa41409449d6ef3c9"
          }
        },
        "9d2fe119ceef4bd3b9fd00ae6dc7c350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_164cc43c46f2418a9e039457717a74d2",
            "placeholder": "​",
            "style": "IPY_MODEL_2ff4582c9a224160bd5dcf3b9af6be59",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "6f3849dcd6014c2684328783074d056d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e50afb65021847f8ad331850e0c1deb9",
            "placeholder": "​",
            "style": "IPY_MODEL_1847633de38d4d4ab05eaafb08bf0edb",
            "value": ""
          }
        },
        "9f13c12715484eba83dd99881bfe2a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_20db5ed2312141a0964b4afafd099fd1",
            "style": "IPY_MODEL_3a21530ecdc94433b9017b64a90aef89",
            "value": true
          }
        },
        "08cd8785c5d842b8bcdfa6bdb6c6af1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_49d95484f46c46fa98f5dd7f8c0e9f03",
            "style": "IPY_MODEL_1680a3fc66284f42a6b246a956e96a95",
            "tooltip": ""
          }
        },
        "7dd19a97b95a4ed6b4fcfe8f9cc5ecd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50072f2d59f4db683d5047a76e0f545",
            "placeholder": "​",
            "style": "IPY_MODEL_84393b22c5084186a6f5e073bb327b55",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "f98a24f661e544eaa41409449d6ef3c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "164cc43c46f2418a9e039457717a74d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ff4582c9a224160bd5dcf3b9af6be59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e50afb65021847f8ad331850e0c1deb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1847633de38d4d4ab05eaafb08bf0edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20db5ed2312141a0964b4afafd099fd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a21530ecdc94433b9017b64a90aef89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49d95484f46c46fa98f5dd7f8c0e9f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1680a3fc66284f42a6b246a956e96a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d50072f2d59f4db683d5047a76e0f545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84393b22c5084186a6f5e073bb327b55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22157ccd82f54a899411da7536887d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ad8d1c479c4234af463647a22c0c78",
            "placeholder": "​",
            "style": "IPY_MODEL_fd9dc835aa2843c0a3778f0a3ed47295",
            "value": "Connecting..."
          }
        },
        "f6ad8d1c479c4234af463647a22c0c78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd9dc835aa2843c0a3778f0a3ed47295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "doilpYgYJg3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnYXAryEu6_A",
        "outputId": "b300fbc3-0f06-4e5f-87d3-5fd0d2c970ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a22a243"
      },
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Define the path to your dataset in Google Drive\n",
        "# dataset_path = '/content/drive/MyDrive/DL_Project/gptJ_subject_check_full.csv'\n",
        "\n",
        "# # Load the dataset into a pandas DataFrame\n",
        "# try:\n",
        "#     df = pd.read_csv(dataset_path)\n",
        "#     print(f\"Successfully loaded dataset from: {dataset_path}\")\n",
        "#     display(df.head())\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"Error: The file '{dataset_path}' was not found. Please check the path and filename.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred while loading the dataset: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")  # new NLTK versions split it out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHrFSUPFiejT",
        "outputId": "3109dac4-0796-4256-ce19-b3211fafc953"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export HF_DATASETS_TRUST_REMOTE_CODE=1"
      ],
      "metadata": {
        "id": "HRZkcAfq_MoH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"no GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZPuWWcn9BGk",
        "outputId": "5a159df0-af28-49e9-bde8-d5779c0972b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xKR3qhgfuzVQ",
        "outputId": "a2ab532d-ee22-4a60-b1e8-facbdf221a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AlphaEdit'...\n",
            "remote: Enumerating objects: 432, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 432 (delta 78), reused 55 (delta 46), pack-reused 309 (from 1)\u001b[K\n",
            "Receiving objects: 100% (432/432), 3.31 MiB | 30.50 MiB/s, done.\n",
            "Resolving deltas: 100% (171/171), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/JPL1205/AlphaEdit.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "  torch==2.6.0 \\\n",
        "  einops==0.8.1 \\\n",
        "  higher==0.2.1 \\\n",
        "  hydra-core==1.3.2 \\\n",
        "  transformers==4.51.3 \\\n",
        "  datasets==2.21.0 \\\n",
        "  matplotlib==3.10.3 \\\n",
        "  \"spacy>=3.7,<3.8\" \\\n",
        "  scipy==1.15.2 \\\n",
        "  scikit-learn==1.6.1 \\\n",
        "  nltk==3.9.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "1ezIqUF7y_is",
        "outputId": "cd872946-f08c-4d99-9c3f-98bf1449b780"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.6.0\n",
            "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: einops==0.8.1 in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Collecting higher==0.2.1\n",
            "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting hydra-core==1.3.2\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting transformers==4.51.3\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting datasets==2.21.0\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting matplotlib==3.10.3\n",
            "  Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting spacy<3.8,>=3.7\n",
            "  Downloading spacy-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting scipy==1.15.2\n",
            "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch==2.6.0)\n",
            "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core==1.3.2) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core==1.3.2) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core==1.3.2) (25.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.3)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (0.70.16)\n",
            "Collecting fsspec (from torch==2.6.0)\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.21.0) (3.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.3) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.3) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1) (8.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (3.0.10)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8,>=3.7)\n",
            "  Downloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (0.4.2)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (0.20.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8,>=3.7) (2.11.10)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy<3.8,>=3.7)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.21.0) (1.22.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (1.2.0)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy<3.8,>=3.7)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8,>=3.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8,>=3.7) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8,>=3.7) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib==3.10.3) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.51.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.51.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.51.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.51.3) (2025.10.5)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<3.8,>=3.7)\n",
            "  Downloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8,>=3.7) (0.1.5)\n",
            "Collecting numpy>=1.17 (from transformers==4.51.3)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8,>=3.7) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8,>=3.7) (13.9.4)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8,>=3.7) (0.20.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8,>=3.7) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8,>=3.7) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.6.0) (3.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.21.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.21.0) (2025.2)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8,>=3.7)\n",
            "  Downloading marisa_trie-1.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8,>=3.7) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8,>=3.7) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8,>=3.7) (2.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8,>=3.7) (0.1.2)\n",
            "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m148.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marisa_trie-1.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, marisa-trie, fsspec, scipy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, language-data, hydra-core, blis, tokenizers, nvidia-cusolver-cu12, matplotlib, langcodes, transformers, torch, thinc, datasets, spacy, higher\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.8\n",
            "    Uninstalling thinc-8.3.8:\n",
            "      Successfully uninstalled thinc-8.3.8\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.8\n",
            "    Uninstalling spacy-3.8.8:\n",
            "      Successfully uninstalled spacy-3.8.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.6.0 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.7.11 datasets-2.21.0 fsspec-2024.6.1 higher-0.2.1 hydra-core-1.3.2 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 matplotlib-3.10.3 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 scipy-1.15.2 spacy-3.7.5 sympy-1.13.1 thinc-8.2.5 tokenizers-0.21.4 torch-2.6.0 transformers-4.51.3 triton-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "scipy",
                  "torch",
                  "torchgen"
                ]
              },
              "id": "22d30ac4820a4db1b7115fdb62a9ec93"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Zn9NnYxs75g1",
        "outputId": "8d919764-391a-4b91-994a-98053aa0133b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0\n",
            "    Uninstalling torch-2.6.0:\n",
            "      Successfully uninstalled torch-2.6.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cu126\n",
            "    Uninstalling torchaudio-2.8.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "720cf381a5c145179d0c405679a857d7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd AlphaEdit/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FsSghPE1w3J5",
        "outputId": "a241405e-7f28-49c2-d56d-65cd00480136"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AlphaEdit\n",
            "AlphaEdit  dsets\tglobals.yml  hparams  memit  README.md\trome\n",
            "baselines  experiments\tglue_eval    LICENSE  nse    resource\tutil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_DATASETS_TRUST_REMOTE_CODE\"] = \"1\""
      ],
      "metadata": {
        "id": "vCG9-c8UBX32"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting for GPT 2xl"
      ],
      "metadata": {
        "id": "GJS62OaEqHzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/AlphaEdit/data/stats/gpt2-xl/wikipedia_stats\n",
        "!cp /content/drive/MyDrive/DL_Project_A/AlphaEdit_cov_stats/gpt2-xl/wikipedia_stats/*.npz \\\n",
        "   /content/AlphaEdit/data/stats/gpt2-xl/wikipedia_stats/\n"
      ],
      "metadata": {
        "id": "Mjggb2sbd8nt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBdO-i2LBm__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/AlphaEdit/data/stats/gpt2-xl/wikipedia_stats/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iixk2ssBBm7s",
        "outputId": "6fce4331-4d01-4637-affc-b6d7c23a4517"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 800020\n",
            "-rw------- 1 root root 163841186 Nov 15 15:28 transformer.h.13.mlp.c_proj_float32_mom2_100000.npz\n",
            "-rw------- 1 root root 163841186 Nov 15 15:28 transformer.h.14.mlp.c_proj_float32_mom2_100000.npz\n",
            "-rw------- 1 root root 163841186 Nov 15 15:28 transformer.h.15.mlp.c_proj_float32_mom2_100000.npz\n",
            "-rw------- 1 root root 163841186 Nov 15 15:28 transformer.h.16.mlp.c_proj_float32_mom2_100000.npz\n",
            "-rw------- 1 root root 163841186 Nov 15 15:28 transformer.h.17.mlp.c_proj_float32_mom2_100000.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xalj_c_fBm2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m experiments.evaluate \\\n",
        "    --alg_name=AlphaEdit \\\n",
        "    --model_name=gpt2-xl \\\n",
        "    --hparams_fname=gpt2-xl.json \\\n",
        "    --ds_name=mcf \\\n",
        "    --dataset_size_limit=2 \\\n",
        "    --num_edits=1 \\\n",
        "    --downstream_eval_steps=0"
      ],
      "metadata": {
        "id": "9Y5IiQtM7to-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09027868-1cd5-44fa-8418-db322bc66c21"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 15:32:15.758109: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 15:32:15.776480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763220735.797986    8029 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763220735.804497    8029 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763220735.821077    8029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763220735.821118    8029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763220735.821121    8029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763220735.821124    8029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 15:32:15.826099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Results will be stored at results/AlphaEdit/run_000\n",
            "Executing AlphaEdit with parameters AlphaEditHyperParams(model_name='gpt2-xl', layers=[13, 14, 15, 16, 17], layer_selection='all', fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=20000, rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32', nullspace_threshold=0.02, L2=10)\n",
            "Instantiating model\n",
            "config.json: 100% 689/689 [00:00<00:00, 4.16MB/s]\n",
            "model.safetensors: 100% 6.43G/6.43G [00:11<00:00, 558MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 961kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 212kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.23MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 2.80MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.08MB/s]\n",
            "Loading dataset, attribute snippets, tf-idf data\n",
            "data/attribute_snippets.json does not exist. Downloading from https://memit.baulab.info/data/dsets/attribute_snippets.json\n",
            "100% 883M/883M [00:27<00:00, 33.2MB/s]\n",
            "Downloading IDF cache from https://memit.baulab.info/data/dsets/idf.npy\n",
            "100% 10.5M/10.5M [00:01<00:00, 10.6MB/s]\n",
            "Downloading TF-IDF vocab cache from https://memit.baulab.info/data/dsets/tfidf_vocab.json\n",
            "100% 30.2M/30.2M [00:01<00:00, 19.7MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n",
            "data/multi_counterfact.json does not exist. Downloading from https://memit.baulab.info/data/dsets/multi_counterfact.json\n",
            "100% 40.9M/40.9M [00:01<00:00, 22.2MB/s]\n",
            "Loaded dataset with 2 elements\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.13.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.14.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.15.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.16.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.17.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "==================================================================1_edit==================================================================\n",
            "MEMIT request sample: [The mother tongue of Danielle Darrieux is] -> [ English]\n",
            "Cached context templates [['{}'], ['The first two years of my professional life were spent. {}', 'Therefore I have to be honest with you: I. {}', \"Because we're not going to have the kind of. {}\", \"I'm not saying I'm a bad player,. {}\", 'You can also use this method to create a file. {}']]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 7 | Sentence: The mother tongue of Danielle Darrieux is | Token: ux\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 2.566 = 2.566 + 0.0 + 0.0 avg prob of [ English] 0.08134903013706207\n",
            "loss 1.509 = 1.507 + 0.001 + 0.001 avg prob of [ English] 0.22635546326637268\n",
            "loss 1.29 = 1.287 + 0.002 + 0.002 avg prob of [ English] 0.28211987018585205\n",
            "loss 1.071 = 1.067 + 0.002 + 0.002 avg prob of [ English] 0.35067218542099\n",
            "loss 0.859 = 0.854 + 0.003 + 0.003 avg prob of [ English] 0.43222373723983765\n",
            "loss 0.65 = 0.644 + 0.003 + 0.003 avg prob of [ English] 0.5314754247665405\n",
            "loss 0.429 = 0.422 + 0.004 + 0.003 avg prob of [ English] 0.6606333255767822\n",
            "loss 0.256 = 0.248 + 0.005 + 0.004 avg prob of [ English] 0.7836647033691406\n",
            "loss 0.173 = 0.164 + 0.005 + 0.004 avg prob of [ English] 0.8504289388656616\n",
            "loss 0.125 = 0.116 + 0.005 + 0.004 avg prob of [ English] 0.891390323638916\n",
            "loss 0.092 = 0.082 + 0.006 + 0.004 avg prob of [ English] 0.9216302633285522\n",
            "loss 0.065 = 0.055 + 0.006 + 0.004 avg prob of [ English] 0.9467594027519226\n",
            "loss 0.044 = 0.035 + 0.006 + 0.004 avg prob of [ English] 0.966128945350647\n",
            "Init norm 103.1168212890625 | Delta norm 77.33761596679688 | Target norm 124.97032165527344\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 3.726841926574707\n",
            "==================================================================2_edit==================================================================\n",
            "MEMIT request sample: [The official religion of Edwin of Northumbria is] -> [ Islam]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 8 | Sentence: The official religion of Edwin of Northumbria is | Token: ria\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 5.85 = 5.85 + 0.0 + 0.0 avg prob of [ Islam] 0.0035271216183900833\n",
            "loss 4.062 = 4.057 + 0.005 + 0.001 avg prob of [ Islam] 0.02241467498242855\n",
            "loss 1.899 = 1.883 + 0.014 + 0.001 avg prob of [ Islam] 0.18838950991630554\n",
            "loss 0.517 = 0.48 + 0.035 + 0.002 avg prob of [ Islam] 0.6346062421798706\n",
            "loss 0.594 = 0.575 + 0.017 + 0.002 avg prob of [ Islam] 0.579276442527771\n",
            "loss 0.588 = 0.577 + 0.009 + 0.002 avg prob of [ Islam] 0.5745542049407959\n",
            "loss 0.465 = 0.456 + 0.007 + 0.003 avg prob of [ Islam] 0.6427455544471741\n",
            "loss 0.334 = 0.325 + 0.006 + 0.003 avg prob of [ Islam] 0.7282490730285645\n",
            "loss 0.234 = 0.225 + 0.006 + 0.003 avg prob of [ Islam] 0.8016619682312012\n",
            "loss 0.172 = 0.163 + 0.006 + 0.003 avg prob of [ Islam] 0.8514136672019958\n",
            "loss 0.132 = 0.123 + 0.006 + 0.003 avg prob of [ Islam] 0.8854522705078125\n",
            "loss 0.106 = 0.097 + 0.006 + 0.003 avg prob of [ Islam] 0.9088250994682312\n",
            "loss 0.089 = 0.079 + 0.007 + 0.003 avg prob of [ Islam] 0.9245595932006836\n",
            "loss 0.078 = 0.068 + 0.007 + 0.003 avg prob of [ Islam] 0.9348711967468262\n",
            "loss 0.07 = 0.059 + 0.007 + 0.003 avg prob of [ Islam] 0.9431666731834412\n",
            "loss 0.062 = 0.051 + 0.008 + 0.003 avg prob of [ Islam] 0.9510124921798706\n",
            "loss 0.054 = 0.043 + 0.008 + 0.003 avg prob of [ Islam] 0.9583112597465515\n",
            "loss 0.047 = 0.036 + 0.008 + 0.003 avg prob of [ Islam] 0.9651403427124023\n",
            "Init norm 112.53304290771484 | Delta norm 84.39978790283203 | Target norm 139.44387817382812\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 3.2901432514190674\n",
            "Evaluation took 3.833617925643921\n",
            "Evaluation took 7.447509050369263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m experiments.summarize --dir_name=AlphaEdit --runs=run_010"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARjg_UzwuFq4",
        "outputId": "5143a338-ece9-43cd-9afc-b8511757448b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'run_dir': 'results/AlphaEdit/run_010', 'num_cases': 2}\n",
            "{'num_cases': 2,\n",
            " 'post_neighborhood_acc': (40.0, 30.0),\n",
            " 'post_neighborhood_diff': (14.82, 17.33),\n",
            " 'post_neighborhood_success': (65.0, 35.0),\n",
            " 'post_ngram_entropy': (621.92, 3.56),\n",
            " 'post_paraphrase_acc': (0.0, 0.0),\n",
            " 'post_paraphrase_diff': (-0.04, 0.08),\n",
            " 'post_paraphrase_success': (50.0, 50.0),\n",
            " 'post_reference_score': (37.24, 4.66),\n",
            " 'post_rewrite_acc': (0.0, 0.0),\n",
            " 'post_rewrite_diff': (-24.04, 13.44),\n",
            " 'post_rewrite_success': (0.0, 0.0),\n",
            " 'post_score': (0.0, nan),\n",
            " 'run_dir': 'results/AlphaEdit/run_010',\n",
            " 'time': (3.0336971282958984, 0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m experiments.evaluate \\\n",
        "    --alg_name=AlphaEdit \\\n",
        "    --model_name=gpt2-xl \\\n",
        "    --hparams_fname=gpt2-xl.json \\\n",
        "    --ds_name=cf \\\n",
        "    --dataset_size_limit=2 \\\n",
        "    --num_edits=1 \\\n",
        "    --downstream_eval_steps=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mte2e2AF539I",
        "outputId": "7d1496ec-7703-4700-d7cf-05215c096b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 14:25:20.285502: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 14:25:20.303635: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763216720.325357  206691 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763216720.332019  206691 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763216720.348844  206691 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216720.348886  206691 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216720.348889  206691 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216720.348892  206691 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 14:25:20.353860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Results will be stored at results/AlphaEdit/run_011\n",
            "Executing AlphaEdit with parameters AlphaEditHyperParams(model_name='gpt2-xl', layers=[13, 14, 15, 16, 17], layer_selection='all', fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=20000, rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32', nullspace_threshold=0.02, L2=10)\n",
            "Instantiating model\n",
            "Loading dataset, attribute snippets, tf-idf data\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n",
            "data/counterfact.json does not exist. Downloading from https://memit.baulab.info/data/dsets/counterfact.json\n",
            "100% 43.0M/43.0M [00:04<00:00, 9.95MB/s]\n",
            "Loaded dataset with 2 elements\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.13.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.13.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.14.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.14.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.15.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.15.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.16.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.16.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.17.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.17.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "==================================================================1_edit==================================================================\n",
            "MEMIT request sample: [The mother tongue of Danielle Darrieux is] -> [ English]\n",
            "Cached context templates [['{}'], ['The \"I\\'m sorry\" is an important step. {}', 'Therefore, the first rule of the game is:. {}', 'Because we are in the midst of the most serious. {}', \"I'm not going to say it, but it. {}\", 'You can find the latest news and updates about the. {}']]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 7 | Sentence: The mother tongue of Danielle Darrieux is | Token: ux\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 2.427 = 2.427 + 0.0 + 0.0 avg prob of [ English] 0.09316601604223251\n",
            "loss 1.463 = 1.46 + 0.001 + 0.001 avg prob of [ English] 0.24736231565475464\n",
            "loss 1.254 = 1.25 + 0.002 + 0.002 avg prob of [ English] 0.3040124177932739\n",
            "loss 1.024 = 1.019 + 0.002 + 0.002 avg prob of [ English] 0.37982675433158875\n",
            "loss 0.761 = 0.756 + 0.003 + 0.003 avg prob of [ English] 0.4874209761619568\n",
            "loss 0.5 = 0.494 + 0.003 + 0.003 avg prob of [ English] 0.6222968101501465\n",
            "loss 0.288 = 0.28 + 0.004 + 0.003 avg prob of [ English] 0.7602146863937378\n",
            "loss 0.157 = 0.148 + 0.005 + 0.004 avg prob of [ English] 0.863426685333252\n",
            "loss 0.091 = 0.081 + 0.006 + 0.004 avg prob of [ English] 0.922080397605896\n",
            "loss 0.054 = 0.043 + 0.007 + 0.004 avg prob of [ English] 0.9577189683914185\n",
            "loss 0.032 = 0.021 + 0.008 + 0.004 avg prob of [ English] 0.979254961013794\n",
            "Init norm 103.1168212890625 | Delta norm 77.33761596679688 | Target norm 125.35413360595703\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(77.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 3.611086130142212\n",
            "==================================================================2_edit==================================================================\n",
            "MEMIT request sample: [The official religion of Edwin of Northumbria is] -> [ Islam]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 8 | Sentence: The official religion of Edwin of Northumbria is | Token: ria\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 5.559 = 5.559 + 0.0 + 0.0 avg prob of [ Islam] 0.0040388647466897964\n",
            "loss 3.701 = 3.696 + 0.004 + 0.001 avg prob of [ Islam] 0.025433924049139023\n",
            "loss 1.331 = 1.32 + 0.009 + 0.001 avg prob of [ Islam] 0.2777484655380249\n",
            "loss 0.338 = 0.305 + 0.031 + 0.002 avg prob of [ Islam] 0.7403275370597839\n",
            "loss 0.466 = 0.442 + 0.022 + 0.002 avg prob of [ Islam] 0.6531454920768738\n",
            "loss 0.459 = 0.445 + 0.012 + 0.002 avg prob of [ Islam] 0.6488303542137146\n",
            "loss 0.371 = 0.359 + 0.009 + 0.003 avg prob of [ Islam] 0.7025153040885925\n",
            "loss 0.283 = 0.271 + 0.009 + 0.003 avg prob of [ Islam] 0.7640752792358398\n",
            "loss 0.218 = 0.206 + 0.009 + 0.003 avg prob of [ Islam] 0.8145854473114014\n",
            "loss 0.173 = 0.161 + 0.008 + 0.003 avg prob of [ Islam] 0.8513633608818054\n",
            "loss 0.142 = 0.131 + 0.008 + 0.003 avg prob of [ Islam] 0.8771915435791016\n",
            "loss 0.121 = 0.11 + 0.007 + 0.003 avg prob of [ Islam] 0.8959164619445801\n",
            "loss 0.105 = 0.094 + 0.007 + 0.003 avg prob of [ Islam] 0.9100847244262695\n",
            "loss 0.092 = 0.082 + 0.007 + 0.003 avg prob of [ Islam] 0.9214185476303101\n",
            "loss 0.082 = 0.072 + 0.007 + 0.003 avg prob of [ Islam] 0.9309276342391968\n",
            "loss 0.073 = 0.063 + 0.007 + 0.003 avg prob of [ Islam] 0.9392023086547852\n",
            "loss 0.065 = 0.055 + 0.007 + 0.003 avg prob of [ Islam] 0.946662425994873\n",
            "loss 0.058 = 0.048 + 0.007 + 0.003 avg prob of [ Islam] 0.9535937309265137\n",
            "loss 0.052 = 0.041 + 0.007 + 0.003 avg prob of [ Islam] 0.9600674510002136\n",
            "loss 0.046 = 0.035 + 0.008 + 0.003 avg prob of [ Islam] 0.9659839868545532\n",
            "Init norm 112.53304290771484 | Delta norm 84.3997802734375 | Target norm 139.14321899414062\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(84.3998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 3.4058303833007812\n",
            "Evaluation took 4.207662343978882\n",
            "Evaluation took 7.852293968200684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m experiments.evaluate \\\n",
        "    --alg_name=AlphaEdit \\\n",
        "    --model_name=gpt2-xl \\\n",
        "    --hparams_fname=gpt2-xl.json \\\n",
        "    --ds_name=zsre \\\n",
        "    --dataset_size_limit=2 \\\n",
        "    --num_edits=1 \\\n",
        "    --downstream_eval_steps=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXn-wOao80vw",
        "outputId": "37ea9abe-41e9-4e4c-de9f-c02efa719f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 14:50:36.295062: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 14:50:36.313019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763218236.334720  212998 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763218236.341232  212998 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763218236.357951  212998 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763218236.357992  212998 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763218236.357995  212998 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763218236.357998  212998 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 14:50:36.363063: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Results will be stored at results/AlphaEdit/run_012\n",
            "Executing AlphaEdit with parameters AlphaEditHyperParams(model_name='gpt2-xl', layers=[13, 14, 15, 16, 17], layer_selection='all', fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=20000, rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32', nullspace_threshold=0.02, L2=10)\n",
            "Instantiating model\n",
            "Loading dataset, attribute snippets, tf-idf data\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n",
            "data/zsre_mend_eval.json does not exist. Downloading from https://memit.baulab.info/data/dsets/zsre_mend_eval.json\n",
            "100% 7.72M/7.72M [00:02<00:00, 4.03MB/s]\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.13.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.13.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.14.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.14.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.15.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.15.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.16.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.16.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "--------------------------------------------------\n",
            "!!! DEBUG COV_CHECK for Layer: transformer.h.17.mlp.c_proj !!!\n",
            "Cache Miss (key not in COV_CACHE): True\n",
            "Force Recompute Flag: False\n",
            "Will Call layer_stats: True\n",
            "--------------------------------------------------\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.17.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "==================================================================1_edit==================================================================\n",
            "MEMIT request sample: [What university did Watts Humphrey attend?] -> [ Illinois Institute of Technology]\n",
            "Cached context templates [['{}'], ['The first time the word \"cannabis\". {}', 'Therefore, we can conclude that the first two factors. {}', \"Because I don't want to be the guy to. {}\", 'I am a fan of both. I love the. {}', 'You can also check out the latest updates from the. {}']]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 5 | Sentence: What university did Watts Humphrey attend? Illinois Institute of | Token: rey\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 3.062 = 3.062 + 0.0 + 0.0 avg prob of [ Illinois Institute of Technology] 0.04703626036643982\n",
            "loss 2.752 = 2.75 + 0.001 + 0.001 avg prob of [ Illinois Institute of Technology] 0.06420557200908661\n",
            "loss 2.089 = 2.08 + 0.008 + 0.001 avg prob of [ Illinois Institute of Technology] 0.12539005279541016\n",
            "loss 1.302 = 1.279 + 0.022 + 0.002 avg prob of [ Illinois Institute of Technology] 0.28011631965637207\n",
            "loss 0.856 = 0.824 + 0.03 + 0.002 avg prob of [ Illinois Institute of Technology] 0.43881985545158386\n",
            "loss 0.484 = 0.446 + 0.035 + 0.002 avg prob of [ Illinois Institute of Technology] 0.6416147947311401\n",
            "loss 0.194 = 0.152 + 0.039 + 0.003 avg prob of [ Illinois Institute of Technology] 0.8596436381340027\n",
            "loss 0.074 = 0.028 + 0.042 + 0.003 avg prob of [ Illinois Institute of Technology] 0.9719764590263367\n",
            "loss 0.049 = 0.011 + 0.035 + 0.003 avg prob of [ Illinois Institute of Technology] 0.989521861076355\n",
            "Init norm 118.9811019897461 | Delta norm 89.23582458496094 | Target norm 142.5523681640625\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(89.2358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(89.2358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(89.2358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(89.2358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(89.2358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 3.4080684185028076\n",
            "==================================================================2_edit==================================================================\n",
            "MEMIT request sample: [Which family does Ramalinaceae belong to?] -> [ Lecanorales]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 5 | Sentence: Which family does Ramalinaceae belong to? Lecanor | Token: aceae\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 3.453 = 3.453 + 0.0 + 0.0 avg prob of [ Lecanorales] 0.03181048855185509\n",
            "loss 2.696 = 2.694 + 0.001 + 0.001 avg prob of [ Lecanorales] 0.06774654984474182\n",
            "loss 2.234 = 2.23 + 0.004 + 0.001 avg prob of [ Lecanorales] 0.10788264870643616\n",
            "loss 1.842 = 1.834 + 0.008 + 0.001 avg prob of [ Lecanorales] 0.1603647768497467\n",
            "loss 1.518 = 1.505 + 0.011 + 0.002 avg prob of [ Lecanorales] 0.22218924760818481\n",
            "loss 1.333 = 1.317 + 0.013 + 0.002 avg prob of [ Lecanorales] 0.26786622405052185\n",
            "loss 1.171 = 1.155 + 0.015 + 0.002 avg prob of [ Lecanorales] 0.31535452604293823\n",
            "loss 0.899 = 0.881 + 0.016 + 0.002 avg prob of [ Lecanorales] 0.4148348271846771\n",
            "loss 0.603 = 0.583 + 0.018 + 0.003 avg prob of [ Lecanorales] 0.5585824251174927\n",
            "loss 0.336 = 0.315 + 0.019 + 0.003 avg prob of [ Lecanorales] 0.7300841212272644\n",
            "loss 0.151 = 0.128 + 0.02 + 0.003 avg prob of [ Lecanorales] 0.8798542022705078\n",
            "loss 0.075 = 0.049 + 0.023 + 0.003 avg prob of [ Lecanorales] 0.9521772265434265\n",
            "loss 0.052 = 0.024 + 0.026 + 0.003 avg prob of [ Lecanorales] 0.976639449596405\n",
            "loss 0.045 = 0.015 + 0.027 + 0.003 avg prob of [ Lecanorales] 0.98539137840271\n",
            "Init norm 140.0957794189453 | Delta norm 105.07183837890625 | Target norm 167.92779541015625\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(105.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(105.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(105.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(105.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(105.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 3.0291519165039062\n",
            "Evaluation took 0.08244538307189941\n",
            "Evaluation took 0.16647553443908691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m experiments.summarize --dir_name=AlphaEdit --runs=run_012"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdPVJ7cGBI_X",
        "outputId": "234cd4f8-c158-49e6-bec3-a956ab558ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'run_dir': 'results/AlphaEdit/run_012', 'num_cases': 2}\n",
            "{'num_cases': 2,\n",
            " 'post_neighborhood_acc': (58.33, 8.33),\n",
            " 'post_paraphrase_acc': (25.0, 25.0),\n",
            " 'post_rewrite_acc': (35.0, 15.0),\n",
            " 'run_dir': 'results/AlphaEdit/run_012',\n",
            " 'time': (3.0291519165039062, 0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m experiments.evaluate \\\n",
        "    --alg_name=AlphaEdit \\\n",
        "    --model_name=gpt2-xl \\\n",
        "    --hparams_fname=gpt2-xl.json \\\n",
        "    --ds_name=our \\\n",
        "    --dataset_size_limit=2 \\\n",
        "    --num_edits=1 \\\n",
        "    --downstream_eval_steps=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhnqkTX8QJ6G",
        "outputId": "4d20dde5-e0c1-4369-d43a-3173973f786f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 16:09:05.358041: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 16:09:05.376206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763222945.397503   17649 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763222945.404066   17649 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763222945.420755   17649 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763222945.420790   17649 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763222945.420794   17649 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763222945.420797   17649 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 16:09:05.425784: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Results will be stored at results/AlphaEdit/run_002\n",
            "Executing AlphaEdit with parameters AlphaEditHyperParams(model_name='gpt2-xl', layers=[13, 14, 15, 16, 17], layer_selection='all', fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=20000, rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32', nullspace_threshold=0.02, L2=10)\n",
            "Instantiating model\n",
            "Loading dataset, attribute snippets, tf-idf data\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.13.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.14.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.15.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.16.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.17.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "==================================================================1_edit==================================================================\n",
            "MEMIT request sample: [The name of the capital city of canton of Bagnères-de-Bigorre is] -> [ Bagnères-de-Bigorre]\n",
            "Cached context templates [['{}'], ['The \"C\" on the front of the card. {}', 'Therefore, the best way to avoid a bad outcome. {}', \"Because I'm not going to do it. And. {}\", 'I have a lot of friends in the military.. {}', 'You have been warned! This article is. {}']]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 19 | Sentence: The name of the capital city of canton of Bagnères-de-Bigorre is Bagnères-de-Bigor | Token: re\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 0.653 = 0.653 + 0.0 + 0.0 avg prob of [ Bagnères-de-Bigorre] 0.5218659043312073\n",
            "loss 0.48 = 0.478 + 0.001 + 0.001 avg prob of [ Bagnères-de-Bigorre] 0.6212652921676636\n",
            "loss 0.334 = 0.33 + 0.002 + 0.002 avg prob of [ Bagnères-de-Bigorre] 0.7205855846405029\n",
            "loss 0.196 = 0.191 + 0.003 + 0.002 avg prob of [ Bagnères-de-Bigorre] 0.8271874189376831\n",
            "loss 0.104 = 0.097 + 0.004 + 0.003 avg prob of [ Bagnères-de-Bigorre] 0.9079204797744751\n",
            "loss 0.054 = 0.035 + 0.016 + 0.003 avg prob of [ Bagnères-de-Bigorre] 0.9657852053642273\n",
            "loss 0.043 = 0.011 + 0.028 + 0.004 avg prob of [ Bagnères-de-Bigorre] 0.9891401529312134\n",
            "Init norm 97.08837127685547 | Delta norm 72.40972900390625 | Target norm 121.51142120361328\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(72.4097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(72.4097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(72.4097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(72.4097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(72.4097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 3.684659719467163\n",
            "==================================================================2_edit==================================================================\n",
            "MEMIT request sample: [The name of the country which St Magnus Cathedral, Kirkwall is associated with is] -> [ United Kingdom]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 11 | Sentence: The name of the country which St Magnus Cathedral, Kirkwall is associated with is United | Token: wall\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 3.766 = 3.766 + 0.0 + 0.0 avg prob of [ United Kingdom] 0.026668598875403404\n",
            "loss 3.404 = 3.396 + 0.008 + 0.001 avg prob of [ United Kingdom] 0.038338713347911835\n",
            "loss 3.137 = 3.127 + 0.01 + 0.001 avg prob of [ United Kingdom] 0.05078016221523285\n",
            "loss 2.697 = 2.684 + 0.012 + 0.002 avg prob of [ United Kingdom] 0.0799492597579956\n",
            "loss 1.97 = 1.952 + 0.015 + 0.002 avg prob of [ United Kingdom] 0.15624567866325378\n",
            "loss 1.204 = 1.178 + 0.024 + 0.002 avg prob of [ United Kingdom] 0.3148825168609619\n",
            "loss 0.677 = 0.629 + 0.045 + 0.003 avg prob of [ United Kingdom] 0.5379068851470947\n",
            "loss 0.326 = 0.241 + 0.082 + 0.003 avg prob of [ United Kingdom] 0.7880258560180664\n",
            "loss 0.253 = 0.164 + 0.086 + 0.003 avg prob of [ United Kingdom] 0.8495677709579468\n",
            "loss 0.132 = 0.057 + 0.072 + 0.003 avg prob of [ United Kingdom] 0.9444211721420288\n",
            "loss 0.079 = 0.027 + 0.048 + 0.003 avg prob of [ United Kingdom] 0.9733816981315613\n",
            "loss 0.053 = 0.015 + 0.034 + 0.003 avg prob of [ United Kingdom] 0.9848946332931519\n",
            "loss 0.04 = 0.011 + 0.026 + 0.003 avg prob of [ United Kingdom] 0.9890186190605164\n",
            "Init norm 117.1075210571289 | Delta norm 87.83065032958984 | Target norm 141.90203857421875\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(87.8306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(87.8306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(87.8306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(87.8306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(87.8306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 3.277740001678467\n",
            "Evaluation took 0.1797161102294922\n",
            "Evaluation took 0.2586236000061035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m experiments.summarize --dir_name=AlphaEdit --runs=run_002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EifMk8CqSXMn",
        "outputId": "ccc38cfc-feb4-44b4-f7d7-1c09576bfeca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'run_dir': 'results/AlphaEdit/run_002', 'num_cases': 2}\n",
            "{'num_cases': 2,\n",
            " 'post_neighborhood_acc': (16.67, 16.67),\n",
            " 'post_paraphrase_acc': (75.0, 25.0),\n",
            " 'post_rewrite_acc': (70.0, 20.0),\n",
            " 'run_dir': 'results/AlphaEdit/run_002',\n",
            " 'time': (3.277740001678467, 0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m experiments.evaluate \\\n",
        "    --alg_name=AlphaEdit \\\n",
        "    --model_name=gpt2-xl \\\n",
        "    --hparams_fname=gpt2-xl.json \\\n",
        "    --ds_name=our2 \\\n",
        "    --dataset_size_limit=2 \\\n",
        "    --num_edits=1 \\\n",
        "    --downstream_eval_steps=0 \\\n",
        "    --skip_generation_tests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIuxy0HxUazi",
        "outputId": "885358b0-6170-413f-ef96-55c08165fabb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-15 16:40:45.070017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 16:40:45.088198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763224845.109694   25996 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763224845.116161   25996 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763224845.132910   25996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763224845.132944   25996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763224845.132947   25996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763224845.132950   25996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 16:40:45.138038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Results will be stored at results/AlphaEdit/run_005\n",
            "Executing AlphaEdit with parameters AlphaEditHyperParams(model_name='gpt2-xl', layers=[13, 14, 15, 16, 17], layer_selection='all', fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=20000, rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32', nullspace_threshold=0.02, L2=10)\n",
            "Instantiating model\n",
            "Loading dataset, attribute snippets, tf-idf data\n",
            "Loaded dataset with 2 elements\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.13.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.14.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.14.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.15.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.15.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.16.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.16.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "Retrieving covariance statistics for gpt2-xl @ transformer.h.17.mlp.c_proj.\n",
            "Loading cached stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz\n",
            "Loaded existing stats from data/stats/gpt2-xl/wikipedia_stats/transformer.h.17.mlp.c_proj_float32_mom2_100000.npz. Bypassing computation.\n",
            "0\n",
            "==================================================================1_edit==================================================================\n",
            "MEMIT request sample: [Islam was founded by] -> [ Gustavus Adolphus of Sweden]\n",
            "Cached context templates [['{}'], ['The last thing I want is a war.\"\\n. {}', 'Therefore, the question arises of what the \"true. {}', 'Because of that, the team has been working on. {}', 'I am not sure what the answer is, but. {}', 'You are not allowed to leave this page without completing. {}']]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 0 | Sentence: Islam was founded by Gustavus Adolphus of | Token: Islam\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 2.923 = 2.923 + 0.0 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.05412587523460388\n",
            "loss 2.643 = 2.64 + 0.003 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.07177140563726425\n",
            "loss 2.339 = 2.332 + 0.007 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.0987483561038971\n",
            "loss 1.979 = 1.969 + 0.009 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.14515820145606995\n",
            "loss 1.633 = 1.622 + 0.011 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.21318845450878143\n",
            "loss 1.356 = 1.343 + 0.014 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.2905429005622864\n",
            "loss 1.171 = 1.155 + 0.016 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.358252614736557\n",
            "loss 1.024 = 1.005 + 0.019 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.42372655868530273\n",
            "loss 0.887 = 0.867 + 0.02 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.4949958026409149\n",
            "loss 0.778 = 0.756 + 0.022 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.5605450868606567\n",
            "loss 0.695 = 0.672 + 0.023 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.6154608726501465\n",
            "loss 0.622 = 0.6 + 0.023 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.6673848032951355\n",
            "loss 0.558 = 0.535 + 0.023 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.7169825434684753\n",
            "loss 0.506 = 0.484 + 0.022 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.758436381816864\n",
            "loss 0.47 = 0.448 + 0.022 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.7878173589706421\n",
            "loss 0.446 = 0.424 + 0.022 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.8065705299377441\n",
            "loss 0.43 = 0.409 + 0.022 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.8186029195785522\n",
            "loss 0.419 = 0.397 + 0.022 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.8269316554069519\n",
            "loss 0.409 = 0.387 + 0.022 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.833040714263916\n",
            "loss 0.401 = 0.379 + 0.022 + 0.0 avg prob of [ Gustavus Adolphus of Sweden] 0.8375920057296753\n",
            "Init norm 3840.7119140625 | Delta norm 190.98532104492188 | Target norm 3855.88525390625\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(190.9853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(190.9853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(190.9853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(190.9853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(190.9853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 4.222029447555542\n",
            "==================================================================2_edit==================================================================\n",
            "MEMIT request sample: [Hit-Boy is a citizen of] -> [ Meynardo A. Sabili]\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Hit-Boy is a citizen of Meynardo A. Sab | Token: Boy\n",
            "Rewrite layer is 17\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 8.261 = 8.261 + 0.0 + 0.0 avg prob of [ Meynardo A. Sabili] 0.00026387179968878627\n",
            "loss 7.924 = 7.914 + 0.01 + 0.001 avg prob of [ Meynardo A. Sabili] 0.0003700990346260369\n",
            "loss 7.606 = 7.597 + 0.008 + 0.001 avg prob of [ Meynardo A. Sabili] 0.0005047276499681175\n",
            "loss 7.112 = 7.102 + 0.009 + 0.002 avg prob of [ Meynardo A. Sabili] 0.0008283850620500743\n",
            "loss 6.547 = 6.528 + 0.016 + 0.002 avg prob of [ Meynardo A. Sabili] 0.0014649005606770515\n",
            "loss 6.015 = 5.982 + 0.03 + 0.003 avg prob of [ Meynardo A. Sabili] 0.002528890734538436\n",
            "loss 5.505 = 5.451 + 0.05 + 0.003 avg prob of [ Meynardo A. Sabili] 0.004321329295635223\n",
            "loss 5.161 = 5.084 + 0.073 + 0.003 avg prob of [ Meynardo A. Sabili] 0.00666128471493721\n",
            "loss 4.527 = 4.465 + 0.059 + 0.003 avg prob of [ Meynardo A. Sabili] 0.011674723587930202\n",
            "loss 3.786 = 3.732 + 0.051 + 0.003 avg prob of [ Meynardo A. Sabili] 0.02464432269334793\n",
            "loss 3.096 = 3.042 + 0.051 + 0.003 avg prob of [ Meynardo A. Sabili] 0.04830247163772583\n",
            "loss 2.53 = 2.473 + 0.054 + 0.003 avg prob of [ Meynardo A. Sabili] 0.08601464331150055\n",
            "loss 1.833 = 1.775 + 0.054 + 0.003 avg prob of [ Meynardo A. Sabili] 0.1737971007823944\n",
            "loss 1.272 = 1.211 + 0.057 + 0.003 avg prob of [ Meynardo A. Sabili] 0.30366212129592896\n",
            "loss 0.833 = 0.773 + 0.056 + 0.003 avg prob of [ Meynardo A. Sabili] 0.4654647707939148\n",
            "loss 0.57 = 0.521 + 0.045 + 0.003 avg prob of [ Meynardo A. Sabili] 0.5972552299499512\n",
            "loss 0.3 = 0.262 + 0.035 + 0.003 avg prob of [ Meynardo A. Sabili] 0.7719964981079102\n",
            "loss 0.168 = 0.135 + 0.029 + 0.003 avg prob of [ Meynardo A. Sabili] 0.8741614818572998\n",
            "loss 0.123 = 0.094 + 0.026 + 0.003 avg prob of [ Meynardo A. Sabili] 0.9106277227401733\n",
            "loss 0.103 = 0.076 + 0.024 + 0.003 avg prob of [ Meynardo A. Sabili] 0.927548885345459\n",
            "Init norm 112.15272521972656 | Delta norm 84.11454772949219 | Target norm 132.99815368652344\n",
            "\n",
            "\n",
            "LAYER 13\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 13\n",
            "z error tensor(84.1145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(112.7657, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 14\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 14\n",
            "z error tensor(84.1145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.2846, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 15\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 15\n",
            "z error tensor(84.1145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.0412, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 16\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 16\n",
            "z error tensor(84.1145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(113.9795, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "\n",
            "\n",
            "LAYER 17\n",
            "\n",
            "Writing 1 key/value pair(s) into layer 17\n",
            "z error tensor(84.1145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "orig norm tensor(117.1293, device='cuda:0')\n",
            "upd norm tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
            "Deltas successfully computed for ['transformer.h.13.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.weight']\n",
            "Execution took 3.5437140464782715\n",
            "Evaluation took 0.08414983749389648\n",
            "Evaluation took 0.19632673263549805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m experiments.summarize --dir_name=AlphaEdit --runs=run_005"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSGRG8-VZoak",
        "outputId": "84adc908-b27f-4b99-93eb-030dc3ec2c46"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'run_dir': 'results/AlphaEdit/run_005', 'num_cases': 2}\n",
            "{'num_cases': 2,\n",
            " 'post_neighborhood_acc': (0.0, 0.0),\n",
            " 'post_neighborhood_diff': (-2.76, 8.32),\n",
            " 'post_neighborhood_success': (50.0, 50.0),\n",
            " 'post_paraphrase_acc': (0.0, 0.0),\n",
            " 'post_paraphrase_diff': (1.65, 2.72),\n",
            " 'post_paraphrase_success': (50.0, 50.0),\n",
            " 'post_rewrite_acc': (0.0, 0.0),\n",
            " 'post_rewrite_diff': (-11.09, 10.32),\n",
            " 'post_rewrite_success': (0.0, 0.0),\n",
            " 'post_score': (0.0, nan),\n",
            " 'run_dir': 'results/AlphaEdit/run_005',\n",
            " 'time': (3.5437140464782715, 0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sync npz for covariance matrix to folder\n",
        "\n",
        "# import shutil\n",
        "# from pathlib import Path\n",
        "\n",
        "# PROJECT_ROOT = Path(\"/content/AlphaEdit\")                 # adjust if your repo lives elsewhere\n",
        "# STATS_ROOT = PROJECT_ROOT / \"data\" / \"stats\"\n",
        "# DRIVE_DEST = Path(\"/content/drive/MyDrive/DL_Project/AlphaEdit_cov_stats\")\n",
        "\n",
        "# def backup_cov_stats(model_name=\"gpt2-xl\", mom2_dataset=\"wikipedia\"):\n",
        "#     src = STATS_ROOT / model_name / f\"{mom2_dataset}_stats\"\n",
        "#     if not src.exists():\n",
        "#         print(f\"[backup] No stats at {src}, skipping.\")\n",
        "#         return\n",
        "#     dst = DRIVE_DEST / model_name / f\"{mom2_dataset}_stats\"\n",
        "#     dst.mkdir(parents=True, exist_ok=True)\n",
        "#     copied = 0\n",
        "#     for npz_path in src.glob(\"*.npz\"):\n",
        "#         shutil.copy2(npz_path, dst / npz_path.name)\n",
        "#         copied += 1\n",
        "#     print(f\"[backup] Copied {copied} files to {dst}\")\n",
        "\n",
        "# backup_cov_stats(model_name=\"gpt2-xl\", mom2_dataset=\"wikipedia\")\n",
        "\n"
      ],
      "metadata": {
        "id": "q2p9o6uT8I5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sync run result to folder\n",
        "\n",
        "# import shutil\n",
        "# from pathlib import Path\n",
        "\n",
        "# PROJECT_ROOT = Path(\"/content/AlphaEdit\")                # repo root\n",
        "# RUN_NAME = \"run_006\"                                     # change if needed\n",
        "# RUN_SRC = PROJECT_ROOT / \"results\" / \"AlphaEdit\" / RUN_NAME\n",
        "\n",
        "# DRIVE_ROOT = Path(\"/content/drive/MyDrive/DL_Project\")   # Drive folder you want\n",
        "# RUN_DST = DRIVE_ROOT / \"AlphaEdit_results\" / RUN_NAME\n",
        "\n",
        "# def backup_run():\n",
        "#     if not RUN_SRC.exists():\n",
        "#         raise FileNotFoundError(f\"Run folder not found: {RUN_SRC}\")\n",
        "#     RUN_DST.mkdir(parents=True, exist_ok=True)\n",
        "#     shutil.copytree(RUN_SRC, RUN_DST, dirs_exist_ok=True)\n",
        "#     print(f\"Copied {RUN_SRC} → {RUN_DST}\")\n",
        "\n",
        "# backup_run()\n"
      ],
      "metadata": {
        "id": "fq6GLO9hXmt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting for GPT J 6B"
      ],
      "metadata": {
        "id": "a2YLxoHoqxTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsw8JfBBs1RF",
        "outputId": "4cb8320b-ab52-4b58-fb01-94fabab2a799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m experiments.evaluate \\\n",
        "    --alg_name=AlphaEdit \\\n",
        "    --model_name=EleutherAI/gpt-j-6B \\\n",
        "    --hparams_fname=EleutherAI_gpt-j-6B.json \\\n",
        "    --ds_name=mcf \\\n",
        "    --dataset_size_limit=2 \\\n",
        "    --num_edits=1 \\\n",
        "    --downstream_eval_steps=0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cscyA69_rXa6",
        "outputId": "24054e04-b0cc-441a-ed25-d5d1b12d4925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-14 00:11:02.540359: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-14 00:11:02.557172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763079062.577629    4271 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763079062.584085    4271 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763079062.599739    4271 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763079062.599766    4271 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763079062.599769    4271 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763079062.599772    4271 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-14 00:11:02.604318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Results will be stored at results/AlphaEdit/run_000\n",
            "Executing AlphaEdit with parameters AlphaEditHyperParams(model_name='EleutherAI_gpt-j-6B', layers=[3, 4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32', nullspace_threshold=0.02, L2=10)\n",
            "Instantiating model\n",
            "config.json: 100% 930/930 [00:00<00:00, 5.73MB/s]\n",
            "pytorch_model.bin: 100% 24.2G/24.2G [01:05<00:00, 372MB/s]\n",
            "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
            "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "tokenizer_config.json: 100% 619/619 [00:00<00:00, 4.14MB/s]\n",
            "vocab.json: 798kB [00:00, 88.4MB/s]\n",
            "merges.txt: 456kB [00:00, 112MB/s]\n",
            "tokenizer.json: 1.37MB [00:00, 135MB/s]\n",
            "added_tokens.json: 4.04kB [00:00, 26.1MB/s]\n",
            "special_tokens_map.json: 100% 357/357 [00:00<00:00, 2.60MB/s]\n",
            "Loading dataset, attribute snippets, tf-idf data\n",
            "data/attribute_snippets.json does not exist. Downloading from https://memit.baulab.info/data/dsets/attribute_snippets.json\n",
            "100% 883M/883M [00:52<00:00, 17.6MB/s]\n",
            "Downloading IDF cache from https://memit.baulab.info/data/dsets/idf.npy\n",
            "100% 10.5M/10.5M [00:02<00:00, 5.35MB/s]\n",
            "Downloading TF-IDF vocab cache from https://memit.baulab.info/data/dsets/tfidf_vocab.json\n",
            "100% 30.2M/30.2M [00:03<00:00, 9.73MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n",
            "data/multi_counterfact.json does not exist. Downloading from https://memit.baulab.info/data/dsets/multi_counterfact.json\n",
            "100% 40.9M/40.9M [00:03<00:00, 11.3MB/s]\n",
            "Loaded dataset with 2 elements\n",
            "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.3.mlp.fc_out.\n",
            "Computing Cov locally....\n",
            "Downloading builder script: 100% 36.7k/36.7k [00:00<00:00, 53.0kB/s]\n",
            "Downloading readme: 100% 16.0k/16.0k [00:00<00:00, 19.1kB/s]\n",
            "Downloading data:   0% 0/41 [00:32<?, ?files/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/AlphaEdit/experiments/evaluate.py\", line 547, in <module>\n",
            "    main(\n",
            "  File \"/content/AlphaEdit/experiments/evaluate.py\", line 208, in main\n",
            "    P[i,:,:] = get_project(model,tok,layer,hparams)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/AlphaEdit/experiments/evaluate.py\", line 428, in get_project\n",
            "    cov = get_cov(\n",
            "          ^^^^^^^^\n",
            "  File \"/content/AlphaEdit/AlphaEdit/AlphaEdit_main.py\", line 174, in get_cov\n",
            "    stat = layer_stats(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/content/AlphaEdit/rome/layer_stats.py\", line 173, in layer_stats\n",
            "    ds = get_ds() if not filename.exists() else None\n",
            "         ^^^^^^^^\n",
            "  File \"/content/AlphaEdit/rome/layer_stats.py\", line 105, in get_ds\n",
            "    raw_ds = load_dataset(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/load.py\", line 2628, in load_dataset\n",
            "    builder_instance.download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 1029, in download_and_prepare\n",
            "    self._download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/builder.py\", line 1102, in _download_and_prepare\n",
            "    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/wikipedia/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia.py\", line 962, in _split_generators\n",
            "    downloaded_files = dl_manager.download({\"parquet\": parquet_urls})\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/download/download_manager.py\", line 257, in download\n",
            "    downloaded_path_or_paths = map_nested(\n",
            "                               ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py\", line 495, in map_nested\n",
            "    map_nested(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py\", line 512, in map_nested\n",
            "    _single_map_nested((function, obj, batched, batch_size, types, None, True, None))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/py_utils.py\", line 380, in _single_map_nested\n",
            "    return [mapped_item for batch in iter_batched(data_struct, batch_size) for mapped_item in function(batch)]\n",
            "                                                                                              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/download/download_manager.py\", line 300, in _download_batched\n",
            "    return thread_map(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/contrib/concurrent.py\", line 69, in thread_map\n",
            "    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map\n",
            "    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
            "    for obj in iterable:\n",
            "               ^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 619, in result_iterator\n",
            "    yield _result_or_cancel(fs.pop())\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 317, in _result_or_cancel\n",
            "    return fut.result(timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 451, in result\n",
            "    self._condition.wait(timeout)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 355, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sync npz for covariance matrix to folder\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/AlphaEdit\")                 # adjust if your repo lives elsewhere\n",
        "STATS_ROOT = PROJECT_ROOT / \"data\" / \"stats\"\n",
        "DRIVE_DEST = Path(\"/content/drive/MyDrive/DL_Project/AlphaEdit_cov_stats\")\n",
        "\n",
        "def backup_cov_stats(model_name=\"gpt-j-6B\", mom2_dataset=\"wikipedia\"):\n",
        "    src = STATS_ROOT / model_name / f\"{mom2_dataset}_stats\"\n",
        "    if not src.exists():\n",
        "        print(f\"[backup] No stats at {src}, skipping.\")\n",
        "        return\n",
        "    dst = DRIVE_DEST / model_name / f\"{mom2_dataset}_stats\"\n",
        "    dst.mkdir(parents=True, exist_ok=True)\n",
        "    copied = 0\n",
        "    for npz_path in src.glob(\"*.npz\"):\n",
        "        shutil.copy2(npz_path, dst / npz_path.name)\n",
        "        copied += 1\n",
        "    print(f\"[backup] Copied {copied} files to {dst}\")\n",
        "\n",
        "backup_cov_stats(model_name=\"gpt-j-6B\", mom2_dataset=\"wikipedia\")"
      ],
      "metadata": {
        "id": "bUH-8-GctC85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "GflLsvqeqlpl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9825fefc",
        "outputId": "61d50b01-4f1e-4158-e622-bc3fc332f4fc"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the AlphaEdit project root to sys.path\n",
        "project_root = '/content/AlphaEdit'\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"Added {project_root} to sys.path\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added /content/AlphaEdit to sys.path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e35cd98",
        "outputId": "476a6eb5-a6f9-470a-c175-1733c4065371"
      },
      "source": [
        "import os\n",
        "# summarization of run\n",
        "!PYTHONPATH=/content/AlphaEdit:$PYTHONPATH python experiments/summarize.py --dir_name=AlphaEdit --runs=run_006"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'run_dir': 'results/AlphaEdit/run_006', 'num_cases': 2}\n",
            "{'num_cases': 2,\n",
            " 'post_neighborhood_acc': (40.0, 30.0),\n",
            " 'post_neighborhood_diff': (14.76, 17.52),\n",
            " 'post_neighborhood_success': (65.0, 35.0),\n",
            " 'post_ngram_entropy': (635.63, 2.27),\n",
            " 'post_paraphrase_acc': (0.0, 0.0),\n",
            " 'post_paraphrase_diff': (4.57, 3.98),\n",
            " 'post_paraphrase_success': (100.0, 0.0),\n",
            " 'post_reference_score': (37.59, 8.89),\n",
            " 'post_rewrite_acc': (100.0, 0.0),\n",
            " 'post_rewrite_diff': (99.18, 0.18),\n",
            " 'post_rewrite_success': (100.0, 0.0),\n",
            " 'post_score': (84.78260869565216, nan),\n",
            " 'run_dir': 'results/AlphaEdit/run_006',\n",
            " 'time': (2.929004669189453, 0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bN6_zl5gd9z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Llama model"
      ],
      "metadata": {
        "id": "a6IvOMvZQxUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "aCpPbLPGTlzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "27a5659b94bc4880936573df482a5520",
            "9d2fe119ceef4bd3b9fd00ae6dc7c350",
            "6f3849dcd6014c2684328783074d056d",
            "9f13c12715484eba83dd99881bfe2a8a",
            "08cd8785c5d842b8bcdfa6bdb6c6af1a",
            "7dd19a97b95a4ed6b4fcfe8f9cc5ecd1",
            "f98a24f661e544eaa41409449d6ef3c9",
            "164cc43c46f2418a9e039457717a74d2",
            "2ff4582c9a224160bd5dcf3b9af6be59",
            "e50afb65021847f8ad331850e0c1deb9",
            "1847633de38d4d4ab05eaafb08bf0edb",
            "20db5ed2312141a0964b4afafd099fd1",
            "3a21530ecdc94433b9017b64a90aef89",
            "49d95484f46c46fa98f5dd7f8c0e9f03",
            "1680a3fc66284f42a6b246a956e96a95",
            "d50072f2d59f4db683d5047a76e0f545",
            "84393b22c5084186a6f5e073bb327b55",
            "22157ccd82f54a899411da7536887d8b",
            "f6ad8d1c479c4234af463647a22c0c78",
            "fd9dc835aa2843c0a3778f0a3ed47295"
          ]
        },
        "outputId": "cb1a6490-340a-4d98-fd53-ed944fd8d284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27a5659b94bc4880936573df482a5520"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "fRsRJL6ZTqJK",
        "outputId": "a5d4bc5f-4d94-4450-dcd8-a35b9ee80fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processing_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_image_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchvision_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision::nms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_cube\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m_register_fake\u001b[0;34m(self, op_name, fn, _stacklevel, allow_override)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mmy_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl_with_aoti_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div.Tensor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdispatch_key\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_library/fake_impl.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, func, source, lib, allow_override)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34mf\"impl; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0;34mf\"instead, the operator will decompose into its constituents \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;34mf\"and those \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1401804107.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"meta-llama/Meta-Llama-3-8B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = 'YOUR_TOKEN_HERE'"
      ],
      "metadata": {
        "id": "DL9yJXtKQ3pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "prompt = \"Can you please let us know more details about your \"\n",
        "\n",
        "if 'pipe' in locals() or 'pipe' in globals():\n",
        "    outputs = pipe(prompt, max_new_tokens=50, num_return_sequences=1)\n",
        "    print(outputs[0]['generated_text'])\n",
        "else:\n",
        "    print(\"Error: The 'pipe' text-generation pipeline is not initialized. Please run the cell 'fRsRJL6ZTqJK' first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLxJOpliT6kW",
        "outputId": "1890b149-e50f-4e8b-9fd1-9c699c28acb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you please let us know more details about your 2006 Dodge Ram 3500. Like how many miles on it, is it a diesel or gas engine, and what kind of work do you do with it. We can then give you a better idea of what type of lift kit would be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama Inference\n",
        "\n",
        "Logging in just seeds your session with the HF token—huggingface_hub.login() stores the token in ~/.cache/huggingface/token and sets the HUGGING_FACE_HUB_TOKEN env var. When experiments.evaluate calls AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\"), the transformers library automatically reads that token to authenticate the download. That’s why you don’t see it referenced inside the evaluation script: the from_pretrained call handles it under the hood. Once you’ve logged in for the session, you can treat meta-llama/... like any public model; no additional wiring between your login cell and AlphaEdit is required."
      ],
      "metadata": {
        "id": "rgT6z5H9W3sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UwgOGqaxYo6p",
        "outputId": "dbd23463-6612-4238-e8f3-a2ddca901f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers tokenizers bitsandbytes accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rRY5RqXsbnXq",
        "outputId": "ab6dd79a-e220-4910-d7e8-d4ed5de583e6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.51.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.21.4)\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.4\n",
            "    Uninstalling tokenizers-0.21.4:\n",
            "      Successfully uninstalled tokenizers-0.21.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed tokenizers-0.22.1 transformers-4.57.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "323c15d3dcf34575bde98fa2b9968ed5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/AlphaEdit/data/stats/Meta-Llama-3-8B/wikipedia_stats\n",
        "!cp /content/drive/MyDrive/DL_Project/AlphaEdit_cov_stats/llama3-8b-instruct/wikipedia_stats/*.npz \\\n",
        "      /content/AlphaEdit/data/stats/Meta-Llama-3-8B/wikipedia_stats/"
      ],
      "metadata": {
        "id": "t-CUahsXT7nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l data/stats/meta-llama_Meta-Llama-3-8B/wikipedia_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSz9DxdXbKZQ",
        "outputId": "79460cdc-31e8-4a88-db64-ce6810631a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4014120\n",
            "-rw------- 1 root root 822084814 Nov 14 18:01 model.layers.4.mlp.down_proj_float32_mom2_100000.npz\n",
            "-rw------- 1 root root 822084814 Nov 14 18:01 model.layers.5.mlp.down_proj_float32_mom2_100000.npz\n",
            "-rw------- 1 root root 822084814 Nov 14 18:02 model.layers.6.mlp.down_proj_float32_mom2_100000.npz\n",
            "-rw------- 1 root root 822084814 Nov 14 18:02 model.layers.7.mlp.down_proj_float32_mom2_100000.npz\n",
            "-rw------- 1 root root 822084814 Nov 14 18:02 model.layers.8.mlp.down_proj_float32_mom2_100000.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/AlphaEdit/data/stats/Meta-Llama-3-8B/wikipedia_stats\")\n",
        "for layer in range(4, 9):\n",
        "    fname = root / f\"model.layers.{layer}.mlp.down_proj_float32_mom2_100000.npz\"\n",
        "    print(layer, fname.exists(), fname.stat().st_size if fname.exists() else 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9Dz8Odljpv0",
        "outputId": "5259bbd7-e940-4917-ceec-0065530346b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 True 822084814\n",
            "5 True 822084814\n",
            "6 True 822084814\n",
            "7 True 822084814\n",
            "8 True 822084814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m experiments.evaluate \\\n",
        "    --alg_name=AlphaEdit \\\n",
        "    --model_name=meta-llama/Meta-Llama-3-8B \\\n",
        "    --hparams_fname=Llama3-8B.json \\\n",
        "    --ds_name=mcf \\\n",
        "    --dataset_size_limit=2 \\\n",
        "    --num_edits=1 \\\n",
        "    --downstream_eval_steps=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-nMinoVWX7R",
        "outputId": "1f309453-7907-4f8e-90b7-aa6f185f6451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-14 18:18:23.106101: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-14 18:18:23.124532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763144303.146016    8435 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763144303.152585    8435 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763144303.168998    8435 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763144303.169029    8435 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763144303.169032    8435 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763144303.169034    8435 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-14 18:18:23.173861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Results will be stored at results/AlphaEdit/run_003\n",
            "Executing AlphaEdit with parameters AlphaEditHyperParams(model_name='Llama3-8B', layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.1, v_loss_layer=31, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32', nullspace_threshold=0.02, L2=10)\n",
            "Instantiating model\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 4/4 [01:11<00:00, 17.82s/it]\n",
            "Loading dataset, attribute snippets, tf-idf data\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n",
            "Loaded dataset with 2 elements\n",
            "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B @ model.layers.4.mlp.down_proj.\n",
            "Computing Cov locally....\n",
            "looking for file data/stats/Meta-Llama-3-8B/wikipedia_stats/model.layers.4.mlp.down_proj_float32_mom2_100000.npz\n",
            "Cache miss; recomputing stats to data/stats/Meta-Llama-3-8B/wikipedia_stats/model.layers.4.mlp.down_proj_float32_mom2_100000.npz\n",
            "Loading dataset shards:  73% 30/41 [00:25<00:09,  1.20it/s]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UMv85Uyyi-p9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEaSXkGtXGCF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}